{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTZgN83d0N2muL9hYDa83j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ambikad04/FL-PrivacyPreserving-HealthcareAnalytics/blob/main/PrivacyPreserving_HealthcareAnalytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries Used\n",
        "\n",
        "The following libraries are essential for implementing the basic neural network classification script:\n",
        "\n",
        "---\n",
        "\n",
        "### Libraries Overview\n",
        "\n",
        "- **`torch`**: The core library for tensor computations, offering support for GPU acceleration and essential operations for deep learning workflows.\n",
        "\n",
        "- **`torch.nn`**: Provides modules and layers to construct and define neural networks (used to build the classification model).\n",
        "\n",
        "- **`torch.optim`**: Contains optimization algorithms such as SGD and Adam, crucial for model training and weight updates.\n",
        "\n",
        "- **`torch.nn.functional`**: Supplies a functional API for operations like activation functions and loss computations.\n",
        "\n",
        "- **`numpy`**: A library for efficient numerical computations and array manipulations.\n",
        "\n",
        "- **`sklearn.datasets.make_classification`**: A utility to generate synthetic datasets for binary or multiclass classification tasks.\n",
        "\n",
        "- **`sklearn.model_selection.train_test_split`**: Splits datasets into training and testing subsets, ensuring reproducible and unbiased evaluations.\n",
        "\n",
        "---\n",
        "\n",
        "These libraries work together to enable the creation, training, and evaluation of a simple neural network for classification on a synthetic dataset."
      ],
      "metadata": {
        "id": "r1odk1iNaip0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S2vKl1aRZtXV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition: Fully Connected Neural Network (FCModel)\n",
        "\n",
        "The `FCModel` is a simple neural network designed for binary classification tasks. It works well with tabular data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Structure**\n",
        "\n",
        "- **Input Layer**:\n",
        "  - Takes input with a size equal to the number of features (`input_dim`).\n",
        "\n",
        "- **Hidden Layers**:\n",
        "  - **First Layer**: 64 neurons with ReLU activation.\n",
        "  - **Second Layer**: 32 neurons with ReLU activation.\n",
        "\n",
        "- **Output Layer**:\n",
        "  - 1 neuron with a sigmoid activation to produce a probability for binary classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "BAfmddcYasMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Fully Connected Model for Tabular Data\n",
        "class FCModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(FCModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))  # Output layer for binary classification\n",
        "        return x"
      ],
      "metadata": {
        "id": "9Ez3YkOkZ5nX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Synthetic Healthcare Data\n",
        "\n",
        "This part shows how to create fake data for a simple classification task, like predicting health conditions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Data Details**\n",
        "\n",
        "- **Features**:\n",
        "  - 100 samples, each with 10 features.\n",
        "  - 8 features are important for making predictions.\n",
        "\n",
        "- **Classes**:\n",
        "  - Two groups (e.g., healthy or unhealthy).\n",
        "\n",
        "- **Splitting**:\n",
        "  - The data is divided into:\n",
        "    - 80% for training the model.\n",
        "    - 20% for testing how well the model works.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This Data?**\n",
        "This data is useful for practicing and testing healthcare-related prediction models."
      ],
      "metadata": {
        "id": "bFNB5U-DcKCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Synthetic Data (Healthcare)\n",
        "X, y = make_classification(n_samples=100, n_features=10, n_informative=8, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "bGZrTe8NaIg_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulating Client Data\n",
        "\n",
        "This section divides the dataset into smaller parts to simulate data for multiple clients, useful for federated learning or distributed training.\n",
        "\n",
        "---\n",
        "\n",
        "### **Data Details**\n",
        "\n",
        "- **Training Data**:\n",
        "  - The training set (`X_train` and `y_train`) is split into 5 clients.\n",
        "  - Each client gets 20 samples of data.\n",
        "\n",
        "- **Testing Data**:\n",
        "  - The testing set (`X_test` and `y_test`) is split into 5 parts.\n",
        "  - Each client gets 5 samples of data for testing.\n",
        "\n",
        "- **Data Format**:\n",
        "  - Each clientâ€™s data is stored as a tuple:\n",
        "    - Features as `torch.tensor` (float32).\n",
        "    - Labels as `torch.tensor` (float32).\n",
        "\n",
        "---\n",
        "\n",
        "Splitting data like this helps simulate scenarios where different clients (e.g., hospitals) have their own local data for training and testing."
      ],
      "metadata": {
        "id": "UyPgLhJIcgM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate Client Data (Split data into 5 clients)\n",
        "clients_data = [(torch.tensor(X_train[i:i+20], dtype=torch.float32), torch.tensor(y_train[i:i+20], dtype=torch.float32)) for i in range(0, 80, 20)]\n",
        "clients_data_test = [(torch.tensor(X_test[i:i+5], dtype=torch.float32), torch.tensor(y_test[i:i+5], dtype=torch.float32)) for i in range(0, 20, 5)]"
      ],
      "metadata": {
        "id": "EkM8IbbEbWyF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Local Model on Each Client\n",
        "\n",
        "This section defines a function to train a model on each client's local data for a given number of epochs.\n",
        "\n",
        "---\n",
        "\n",
        "### **Function Details**\n",
        "\n",
        "- **Inputs**:\n",
        "  - `model`: The neural network model to be trained.\n",
        "  - `data`: The input features for training.\n",
        "  - `targets`: The actual labels or targets for the data.\n",
        "  - `epochs`: The number of times to iterate through the dataset (default is 5).\n",
        "\n",
        "- **Training Process**:\n",
        "  - **Loss Function**: Binary Cross-Entropy Loss (`BCELoss`) is used, suitable for binary classification.\n",
        "  - **Optimizer**: Stochastic Gradient Descent (SGD) is used with a learning rate of 0.01 to adjust the model's parameters.\n",
        "  - The model is set to training mode (`model.train()`).\n",
        "  - For each epoch, gradients are reset, the model makes predictions, the loss is calculated, and gradients are updated.\n",
        "\n",
        "- **Output**:\n",
        "  - The function returns the model's updated state (the trained model parameters).\n",
        "\n",
        "---\n",
        "\n",
        "Training locally helps simulate how different clients can independently train models using their private data in federated learning."
      ],
      "metadata": {
        "id": "ekY7Xrxmclls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Local Model on Each Client\n",
        "def train_local_model(model, data, targets, epochs=5):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model.state_dict()"
      ],
      "metadata": {
        "id": "-2kxYqOZaQV3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Averaging with Personalization\n",
        "\n",
        "This part defines a function to update the global model by averaging the weights from local models and adding some personalization.\n",
        "\n",
        "---\n",
        "\n",
        "### **Function Details**\n",
        "\n",
        "- **Inputs**:\n",
        "  - `global_model`: The main model that will be updated.\n",
        "  - `local_models_weights`: A list of weights from the models trained by different clients.\n",
        "  - `personalization_factor`: A value (default 0.1) that controls how much each local model affects the global model.\n",
        "\n",
        "- **Process**:\n",
        "  - The function starts by getting the current weights of the global model.\n",
        "  - **Averaging**: For each weight in the global model:\n",
        "    - It calculates the average of the weights from all local models.\n",
        "    - Then, it updates this average by adding a small part of the local model's weights, based on the personalization factor.\n",
        "  - The updated weights are then set back into the global model.\n",
        "\n",
        "- **Output**:\n",
        "  - The global model is updated with the combined and personalized weights from the local models.\n",
        "\n",
        "---\n",
        "\n",
        "Personalization helps each client's model keep some of its own data's characteristics, while still learning from the global model. This is helpful when clients have different kinds of data."
      ],
      "metadata": {
        "id": "BgDsCQtCcwlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Federated Averaging (with Personalization)\n",
        "def federated_averaging_with_personalization(global_model, local_models_weights, personalization_factor=0.1):\n",
        "    global_state_dict = global_model.state_dict()\n",
        "    for key in global_state_dict:\n",
        "        global_state_dict[key] = torch.mean(torch.stack([local_weights[key] for local_weights in local_models_weights]), dim=0)\n",
        "        for local_weights in local_models_weights:\n",
        "            global_state_dict[key] = global_state_dict[key] * (1 - personalization_factor) + local_weights[key] * personalization_factor\n",
        "    global_model.load_state_dict(global_state_dict)"
      ],
      "metadata": {
        "id": "mPwij4AtbgBq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the Global Model\n",
        "\n",
        "This section initializes the global model, which will be used in the federated learning process.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps**\n",
        "\n",
        "- **Input Dimension**:\n",
        "  - The `input_dim` is set to the number of features in the training data (`X_train`).\n",
        "\n",
        "- **Global Model**:\n",
        "  - A new instance of the `FCModel` is created using the input dimension, which will define the structure of the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Purpose**\n",
        "This global model will be updated by combining the knowledge from different local models trained by clients."
      ],
      "metadata": {
        "id": "t81azGTcdO0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Global Model\n",
        "input_dim = X_train.shape[1]  # Number of features in the dataset\n",
        "global_model = FCModel(input_dim)\n"
      ],
      "metadata": {
        "id": "XXgQ2Ow9aW-v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Federated Learning Process (10 Rounds)\n",
        "\n",
        "This section describes the process of training a global model over 10 rounds using federated learning.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps**\n",
        "\n",
        "1. **For Each Round (10 rounds)**:\n",
        "   - Print the round number (e.g., \"Round 1\").\n",
        "   - Create an empty list to store the local model weights from each client.\n",
        "\n",
        "2. **Local Model Training (For Each Client)**:\n",
        "   - Each client uses their own data to train a local model.\n",
        "   - The local model is initialized with the global model's weights.\n",
        "   - The local model is trained on the client's data using the `train_local_model` function.\n",
        "   - The trained model's weights are added to the list of local weights.\n",
        "\n",
        "3. **Federated Averaging with Personalization**:\n",
        "   - The `federated_averaging_with_personalization` function updates the global model by combining the local model weights, while adding some personalization.\n",
        "\n",
        "4. **Testing the Global Model (Optional)**:\n",
        "   - After updating the global model, it is tested on the test data from each client.\n",
        "   - The model makes predictions, and the accuracy is calculated based on how many predictions match the true labels.\n",
        "\n",
        "5. **Repeat for 10 Rounds**:\n",
        "   - This process is repeated for 10 rounds, with the global model being updated and tested after each round.\n",
        "\n",
        "---\n",
        "\n",
        "### **Purpose**\n",
        "This process simulates how federated learning works by training and updating the global model using data from multiple clients, while testing its performance after each round."
      ],
      "metadata": {
        "id": "OwCdRPbIdXS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Federated Learning Process (10 rounds)\n",
        "for round in range(10):\n",
        "    print(f\"Round {round + 1}\")\n",
        "    local_models_weights = []\n",
        "\n",
        "    # Each client trains on its local data\n",
        "    for client_data, client_targets in clients_data:\n",
        "        local_model = FCModel(input_dim)\n",
        "        local_model.load_state_dict(global_model.state_dict())  # Initialize with global model weights\n",
        "        local_weights = train_local_model(local_model, client_data, client_targets)\n",
        "        local_models_weights.append(local_weights)\n",
        "\n",
        "    # Federated Averaging with Personalization\n",
        "    federated_averaging_with_personalization(global_model, local_models_weights)\n",
        "\n",
        "    # Testing the model on client data (optional)\n",
        "    global_model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for client_data, client_targets in clients_data_test:\n",
        "            outputs = global_model(client_data)\n",
        "            predicted = (outputs.squeeze() > 0.5).float()\n",
        "            total += client_targets.size(0)\n",
        "            correct += (predicted == client_targets).sum().item()\n",
        "        accuracy = correct / total\n",
        "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzAegcenbni_",
        "outputId": "2c2629cf-5cbb-4c6f-bca3-663ec34f1b56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1\n",
            "Test Accuracy: 75.00%\n",
            "Round 2\n",
            "Test Accuracy: 70.00%\n",
            "Round 3\n",
            "Test Accuracy: 80.00%\n",
            "Round 4\n",
            "Test Accuracy: 85.00%\n",
            "Round 5\n",
            "Test Accuracy: 75.00%\n",
            "Round 6\n",
            "Test Accuracy: 75.00%\n",
            "Round 7\n",
            "Test Accuracy: 75.00%\n",
            "Round 8\n",
            "Test Accuracy: 80.00%\n",
            "Round 9\n",
            "Test Accuracy: 80.00%\n",
            "Round 10\n",
            "Test Accuracy: 80.00%\n",
            "Training complete.\n"
          ]
        }
      ]
    }
  ]
}